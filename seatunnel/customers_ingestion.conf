env {
  execution.parallelism = 2
  job.mode = "BATCH"
  spark.master = "spark://spark-master:7077"
}

source {
  LocalFile {
    path = "/opt/data/raw/customers.csv"
    file_format_type = "csv"
    schema = {
      fields {
        customer_id = string
        name = string
        date_of_birth = string
        address = string
        city = string
        state = string
        zip_code = string
        phone = string
        email = string
        annual_income = double
        job_title = string
        employment_status = string
        marital_status = string
        created_date = string
      }
    }
    result_table_name = "customers_raw"
  }
}

transform {
  Sql {
    sql = "
      SELECT 
        customer_id,
        name,
        CAST(date_of_birth AS DATE) as date_of_birth,
        address,
        city,
        state,
        zip_code,
        phone,
        email,
        CAST(annual_income AS DECIMAL(12,2)) as annual_income,
        job_title,
        employment_status,
        marital_status,
        CAST(created_date AS DATE) as created_date,
        CURRENT_TIMESTAMP as ingestion_timestamp
      FROM customers_raw
      WHERE customer_id IS NOT NULL
        AND name IS NOT NULL
    "
    result_table_name = "customers_clean"
  }
}

sink {
  Jdbc {
    url = "jdbc:postgresql://postgres:5432/customer360_dw"
    driver = "org.postgresql.Driver"
    user = "postgres"
    password = "postgres"
    query = "INSERT INTO staging.customers (customer_id, name, date_of_birth, address, city, state, zip_code, phone, email, annual_income, job_title, employment_status, marital_status, created_date, ingestion_timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
    source_table_name = "customers_clean"
  }
}
