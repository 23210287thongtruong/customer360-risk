env {
  execution.parallelism = 2
  job.mode = "BATCH"
  spark.master = "spark://spark-master:7077"
}

source {
  LocalFile {
    path = "/opt/data/raw/credit_scores.csv"
    file_format_type = "csv"
    schema = {
      fields {
        customer_id = string
        credit_score = int
        score_date = string
        credit_history_length = int
        number_of_accounts = int
        total_debt = double
        credit_utilization = double
      }
    }
    result_table_name = "credit_scores_raw"
  }
}

transform {
  Sql {
    sql = "
      SELECT 
        customer_id,
        credit_score,
        CAST(score_date AS DATE) as score_date,
        credit_history_length,
        number_of_accounts,
        CAST(total_debt AS DECIMAL(12,2)) as total_debt,
        CAST(credit_utilization AS DECIMAL(5,4)) as credit_utilization,
        CURRENT_TIMESTAMP as ingestion_timestamp
      FROM credit_scores_raw
      WHERE customer_id IS NOT NULL
        AND credit_score BETWEEN 300 AND 850
        AND credit_utilization BETWEEN 0 AND 1
    "
    result_table_name = "credit_scores_clean"
  }
}

sink {
  Jdbc {
    url = "jdbc:postgresql://postgres:5432/customer360_dw"
    driver = "org.postgresql.Driver"
    user = "postgres"
    password = "postgres"
    query = "INSERT INTO staging.credit_scores (customer_id, credit_score, score_date, credit_history_length, number_of_accounts, total_debt, credit_utilization, ingestion_timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?)"
    source_table_name = "credit_scores_clean"
  }
}
