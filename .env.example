# Environment Variables for Customer 360 Risk Scoring Project
# Copy this file to .env and fill in your actual values

# Database Configuration
POSTGRES_DB=customer360_dw
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_postgres_password_here
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Airflow Configuration
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:your_secure_postgres_password_here@postgres:5432/customer360_dw
AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here_generate_with_python_cryptography
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true

# Data Pipeline Paths (used by DAG functions)
AIRFLOW_SCRIPTS_DIR=/opt/airflow/scripts
AIRFLOW_DATA_DIR=/opt/airflow/data/raw

# Spark Configuration
SPARK_MASTER_URL=spark://spark-master:7077
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2

# Data Generation Configuration
DEFAULT_CUSTOMERS=10000
DEFAULT_TRANSACTIONS_PER_CUSTOMER=50
DATA_OUTPUT_DIR=./data/raw

# Metabase Configuration
MB_DB_TYPE=postgres
MB_DB_DBNAME=customer360_dw
MB_DB_PORT=5432
MB_DB_USER=postgres
MB_DB_PASS=your_secure_postgres_password_here
MB_DB_HOST=postgres

# Jupyter Configuration
JUPYTER_ENABLE_LAB=yes

# Pipeline Configuration
PIPELINE_SCHEDULE=@daily
PIPELINE_MAX_ACTIVE_RUNS=1
PIPELINE_RETRIES=2
PIPELINE_RETRY_DELAY=5